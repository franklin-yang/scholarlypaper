\section{Discussion}
We develop a preliminary study of ambiguity as a medium for eliciting bias and find that we fail to discover bias in our QA models using selectional ambiguity but do discover gender bias using retrieval ambiguity. 
We find that, when answering unrestricted ambiguous questions, retriever models amplify gender bias found in Wikipedia, especially when compared with reader models. 
Our ability to elicit bias by easing restrictions on ambiguity follows patterns from psychology~\cite{felson1981ambiguity}, where increased ambiguity in questions allows for improved probing of bias. 
\subsection{Future Work}
We view our work as a preliminary inquiry into ambiguity and bias, leaving deeper investigations as future work.

\paragraph{Additional Experiments}
It would be interesting to see how bias varies based on the phrasing of ambiguous questions, along with the use of a wider variety of models and retrievers. 
Training sets for language models inevitably affect the presence of biases; future investigations can see if the prevalence or existence of gender biases differs between models trained on news articles vs. Wikipedia datasets. 
Additionally, are models trained only on male entities perform poorly when answering questions about female entities? 
\nrcomment{Should we include this, and reword this}
The use of ambiguity as a revelatory mechanism can also be extended to image-based applications, such as blurring images used in visual question answering to detect racial biases in image-based systems. 

\paragraph{Combating Inequity with QA Models}
\label{sec:combat}
Representational harms are perpetuated even in the absence of QA systems, but these models refract pre-existing biases from training data into a new medium (Section~\ref{sec:refraction}).
If inequity is light traveling through water, this new medium may speed it up like air or slow it down like glass.
Considering a counterfactual world where QA models do not exist, inequity, therefore, remains present.
As we grow aware of how machine learning can combat as well as perpetuate harms, we must also develop normative goals and ideas for future systems.
One approach could be reconsidering how models should best answer ambiguous or uncomfortable questions.
Rather than abstaining from answering these questions, models could mimic human teacher or parent responses to teach the question asker and guide future inquiries.
While our work focuses on the immediate and pressing goal of developing metrics to ensure systems do not amplify existing inequity, an ideal question answering system does not just turn a blind eye to the mistakes of the past but corrects them.

\subsection{Threats to Validity}
While we aimed to select a diverse cohort of QA models, our studies are limited to only three types of models and one retriever.  
Additionally, we might be better able to probe QA systems by switching from straightforward questions (``Who discovered the Biot-Savart law'') to more nuanced questions involving complex logic or paraphrasing (``Who discovered the law describing the magnetic field generated by electric current''). 
The inclusion of these types of questions might require more powerful QA models; we tried testing these types of questions but our QA models failed to answer them correctly with any regularity. 
\nrcomment{Does this sound good?}
Our reliance on a gender-guesser is also potentially troublesome because of cultural biases in gender guessers; we could have instead used nationality-based gender-guessers~\cite{vasilescu2014gender} to determine gender more accurately. 


% Our experiments repeatedly ran into the problem of imbalanced data. 

% We note that even if the model is not biased, a bias towards certain genders and occupations manifests itself in the underlying dataset, which can easily be revealed via ambiguous questions. Models deployed into the wild could reinforce stereotyping associations.
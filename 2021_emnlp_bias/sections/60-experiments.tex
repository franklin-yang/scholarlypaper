\section{Experiments}
We apply our bias metrics on three QA readers, each trained on the SQuAD dataset~\cite{rajpurkar2016squad}, and the Dense Passage Retriever \cite{karpukhin2020dense}. 
\subsection{Experimental Setup}
We develop three question-answering models---LSTM, BiDAF~\cite{seo2016bidirectional}, and BERT~\cite{devlin2018bert}---and test for bias in each of the models using selectional ambiguity~(Section \ref{sec:bias}). 
% For retrieval ambiguity, we combine a DPR retrieval system with a BLINK QA system to test for distributional shifts~\cite{karpukhin2020dense}. 
We use prior implementations for BiDAF~\cite{chrischute} and BERT~\cite{wolf2019huggingface} and implement our own LSTM model. 
We train all QA readers using the SQuAD dataset~\cite{rajpurkar2016squad}.
For the LSTM and BiDAF models, we convert questions and contexts into GloVe embeddings~\cite{pennington2014glove}, while for BERT, we use the BERT tokenizer~\cite{wolf2019huggingface}. 
% The outputs for all three models are two vectors representing confidence scores for the start and end of the answer span. 
We evaluate models using exact match (EM), F1, and answer vs. no answer (AvNA) scores.  
% EM determines if predicted and actual spans are the same, F1 scores look at the length of intersection between predicted and actual spans, and AvNA scores calculate the rate of abstinence 
(Table~\ref{tab:models}). 


\begin{table*}[]
	\begin{center}
		\begin{tabular}{llll}
			
			Model                    & EM    & F1    & AvNA  \\ \hline
			LSTM 					 & 56.95 & 60.39 & 67.05 \\ 
			BiDAF                    & 57.2  & 60.5  & 67.5  \\ 
			BERT		             & \textbf{70.8}  & \textbf{73.9}  & \textbf{50.1}  \\ \hline
		\end{tabular}
		\caption{\label{tab:models} Accuracy metrics on the SQuAD 2.0 dev set. Despite outperforming the other two models on all accuracy metrics and answering more frequently, BERT scores similarly in bias metrics}
	\end{center}
	
\end{table*}

% Training was performed on a single GeForce GTX 1080 with 12GB memory on RHEL Linux.
% Additionally, for LSTM and BiDAF, we use the following hyperparameters: learning rate: 0.5, optimizer: Adadelta~\cite{zeiler2012adadelta}, dropout rate: 0.2 (all layers); we clip gradients with norm greater than 5, and for BERT, we used the default hyperparameters. 


\begin{table}[]
	\begin{tabular}{lll}
		
		Model                          & accuracy (M) & accuracy (F) \\ \hline
		LSTM  & 0.694        & 0.643        \\ 
		BiDAF                          & 0.697         & 0.687         \\ 
		BERT                  & 0.391         & 0.399         \\ 
	\end{tabular}
	\caption{\label{tab:select} For selectional ambiguity questions, we plot the recall for male and female names. We find little to no difference between male and female recall for all three models.}
\end{table}


\subsection{Selectional Ambiguity}
We compare the recall rates for male and female names on the selectional ambiguity benchmark~\ref{tab:select}. 
We find that recall for male and female names are similar for all three models, indicating that the selectional ambiguity questions were unable to elicit gender bias. 
This is potentially due to the simple nature of the questions; QA models were simply asked to perform reading comprehension rather than retrieval, which may limit the expression of model bias.
Even with the second batch of questions that were consistent with the Cranfield perspective, no significant difference was found at a p=0.05 significance level. 
To confirm male and female retrieval rate similarity, we run a chi-squared test of significance and find little difference between male and female retrieval rates. 
We compare the relative span probabilities between male and female entities in the answer context, and find little that females outscore males 88 to 83 across 171 questions, a rate that suggests little evidence against the null hypothesis of a random coin flip for each question.

\subsection{Retrieval Ambiguity}
\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.7]{\figfile{DPR_gender_diff.png}}
	\caption{Gender disparity for the eight most male and most female jobs. A positive score means a higher number of females were represented from chance, and vice versa for males. We find that stereotypically female roles have a higher score, such as nurse and dancer.}
\end{figure*}

We run experiments using the DPR retriever and reader~\cite{karpukhin2020dense}.
% We test for bias in open-domain QA systems by analyzing retrieval frequencies using the retrieval ambiguity questions, which contain questions of the form ``Who was an engineer?''.
% \mscomment{SHould there be a period here??}
Our retrieval ambiguity question set consists of seventy questions from UNQOVER occupations and 721 questions generated from the QANTA dataset, each associated with an occupation. For each question, we retrieve one hundred passages from Wikipedia. 
We compute the number of passages belonging to a male biography and likewise for female biographies. We define the gender disparity for an occupation as the difference between the number of male and female passages.
We plot the eight lowest and highest gender disparities and find a significant gender skew by occupation aligning with common stereotypes of males and females~\cite{genderstereotype}. 
For example, stereotypically female occupations such as nurse and dancer were skewed towards women, and occupations like astronaut were skewed towards men. 

We run a chi-square goodness-of-fit test between the gender frequencies of the retriever and the gender frequencies of biographies in Wikipedia \cite{maher2018wikipedia} and find significance at the p=0.05 level, supporting the idea that the retriever retrieves significantly more passages of males.  Considering occupations individually, we find significant differences between the distribution of genders retrieved and those occurring in Wikidata, with a sample of 5 such occupations shown in Table 6.
We use retriever predictions as context for a BERT reading comprehension model.
Out of seventy questions, fifty-two responses were male, six were female, and twelve were gender-neutral, which is similar to the $17\%$ of Wikipedia biographies that are women~\cite{maher2018wikipedia}, giving evidence to the idea that retrievers propagate bias at a level more than what's present in the real world, while readers might not. 
\nrcomment{What percent retrieved are female?}
\mscomment{Re: these stereotypes, are they historical or current, does it vary based on time?}

We run a fisher's exact test between the gender frequencies oaf the retriever and the gender frequencies of entities retrieved from QANTA for different occupations and nationalities, and find significance at the p=0.05 level.
We perform gender frequency analysis for occupations alone, with the occupations that differ significantly presented in Table 4.
To perform more focused analysis, we then repeat the frequency analysis, but instead of considering, for example, the gender distribution of entities in a particular occupation, we examine the gender distribution for persons of a particular occupation and nationality combination.  We find significance at the p=0.05 level for multiple combinations, with a sample of 10 being displayed in Table 5.

\begin{table}[]
	\begin{tabular}{ll}
	occupation & \begin{tabular}[c]{@{}l@{}}statistic\end{tabular} \\
		\hline
		saint & 10.9 \\
musician & inf \\
atheist & 0.0 \\
mathematician & 0.0 \\
member & inf \\
rapper & inf \\
scientist & 0.0 \\
hero & 0.0 \\
choreographer & inf \\
sculptor & 0.0 \\
head & inf \\
anthropologist & 0.0 \\
	\end{tabular}
	\caption{\label{tab:select} These occupations resulted in significantly different gender distributions between the QANTA dataset and the retriever results}
\end{table}


\begin{table}[]
	\begin{tabular}{ll}
		
occupation &\begin{tabular}[c]{@{}l@{}}statistic\end{tabular} \\ \hline
Russian Empire figure & inf \\
Republic of Ireland philosopher & inf\\
United States of America choreographer & inf \\
Trinidad and Tobago person & 0\\
United States of America figure & inf\\
Denmark author & inf\\
United States of America anthropologist & 0\\
Duchy of Aquitaine person & inf\\
Spain character& inf\\
New Zealand author & inf\\\hline
	\end{tabular}
	\caption{\label{tab:select} These occupations and ethnicity combinations represent a subsample of those that resulted in significantly different gender distributions between the QANTA dataset and the retriever results}
\end{table}

\begin{table}[]
	\begin{tabular}{ll}
		
occupation &\begin{tabular}[c]{@{}l@{}} statistic\end{tabular} \\\hline
		
lifeguard & inf\\
marine & inf\\
technician & inf\\
politician & inf\\
astronaut & 94.0 \\
	\end{tabular}
	\caption{\label{tab:select} These occupations from the UNQOVER dataset represent a subsample of those that resulted in significantly different gender distributions between Wikidata and the Dense Passage Retriever results}
\end{table}



Our results indicate that closed-domain ambiguous questions are not able to elicit bias as defined in this study, while retrieved ambiguity open-domain questions can give insight into bias in retriever models.  When asking questions given a context that implies multiple correct responses, reader models failed to consistently bias one gender over another in a statistically significant manner, regardless of the complexity of the model.  The Dense Passage Retriever however returned results that differed significantly in distribution when compared to querying Wikidata or the QANTA dataset.
Further work is necessary to understand whether retrievers propagate bias at a higher rate than readers, and if so, why.




